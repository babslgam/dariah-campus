---
uuid: lfEtcr9YiK6Qco7N2M9ZI
title: Improving quality assurance and development efficiency with regression testing
date: 2024-11-15
authors:
  - krautgartner-barbara
categories:
  - acdh-ch
tags:
  - software-development
abstract: Short teaser about the contents of your resource.
type: training-module
licence: ccby-4.0
toc: true
targetGroup: Developers, Project Managers, Test Managers
domain: Social Sciences and Humanities
lang: en
version: 1.0.0
---

 **_NOTE:_** Target audience: Developers, Project Managers, Test Managers

## Introduction
Developing applications in the field of humanities sometimes means rendering a lot of textual data with regards to scientific rules (e.g. citation patterns).
Especially digital editions can be very challenging.

The following sections try to give an insight into the benefits of regression testing using the example of migrating a digital edition to a slightly different tech stack. In this scenario e2e testing is used to make sure that the new application displays the data after the migration exactly as it did before.

The tool of choice in this example is the [Playwright](https://playwright.dev/) framework, which allows us running multiple browser interactions and comparing the result to our test data. By parameterizing the tests we can speed up the development process and achieve high test coverage. The usefulness of setting up these tests becomes evident looking at the quantity and quality of the data in our use case.

The tech stack is pretty straight forward: XML gets transformed to HTML by using XSLT. 
XML structures can be rather simple and flat. But in the case of digital editions the complexity increases based on the levels of nesting and the amount of enriched data combined with a sophisticated rule set regarding the desired display.


In the following two types of tests will by presented: Comparison by Screenshot and Text Matching.

## Learning Outcomes

After reading this post, developers, test and project managers in the field of digital humanities will be able to:

- describe which kind of e2e tests are beneficial in the field of digital editions.
- assess how those kind of tests can be used to avoid regressions in their own projects.


## Screenshot Comparison - Testing the editions main view containing the running text

This article will not go into the specifics of the encoding schema of the edition, but here is a list of some criteria the rendering has to meet:
  - certain elements should not be rendered at all
  - certain elements should be rendered at a different position
  - for certain elements different font and formatting rules have to be applied

### Test object

The edition view consists of 65 html pages. One step of the migration process consists of simplifying certain parts of the code base. (f. e. simpler, more compact xslt and css rules). 
[Page 18](https://karl-kraus.github.io/wpn-static/absatz_18.html) of the edition gives a good example of the complexity that has to be dealt with.

### Testing method

In Playwright [Visual comparisons](https://playwright.dev/docs/test-snapshots) provide the ability to take browser screenshots and compare them. Usually the developer would work on a view of his app and run an initial screenshot comparison test - which fails expectedly, because there are no screenshots yet - to produce reference screenshots which are then used when rerunning the test. In the presented use case, the initial screenshots are taken from the website that has to be migrated. During the migration process the tests can then be rerun at any time to compare the current rendering to the original application.
Running tests generates a test report. This report can come in different formats based on the chosen [reporter](https://playwright.dev/docs/test-reporters). 
In the presented use case the [HTML Reporter](https://playwright.dev/docs/test-reporters#html-reporter) is used, which produces two different report views.
The first view displays all completed tests (failed and passed) in list form: 
<Figure src="images/playwright-list-view.png">
Playwright HTML Reporter List View
</Figure>
From the list view, the detail view of a test can be accessed. If a test failed this detail view contains handy sub views including a side-by-side comparison as well as a "slider": 
<video src="/assets/videos/playwright-test-report-screenshot-comparison_converted.mp4" width="100%" height="auto" controls preload="none">
</video>


Testing by comparing screenshots not only allows faster code refactoring, but also builds an important foundation for future changes, which can also occur in the edition data itself.

## Text Comparison - Testing the editions detail views containing meta information

### Test object

Next to the structural markup the edition contains annotations that point to metadata.


<table>
<caption>References to metadata as of November 2024:</caption>
  <thead>
    <tr>
      <th colSpan="2">annotated encodings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>persons, person groups</td>
      <td>899</td>
    </tr>
    <tr>
      <td>intertexts</td>
      <td>1950</td>
    </tr>
    <tr>
      <td>comments</td>
      <td>766</td>
    </tr>
  </tbody>
</table>


The metadata is displayed next to the main text in the edition view in two different modes/views: a short info view (visible after clicking on the element in the text) and a detail view (visible after clicking on the short info).

<Figure src="images/short_info_wpn.png">
Karl Kraus 1933: Dritte Walpurgisnacht — edition view with short info
</Figure>

<Figure src="images/detail_info_wpn.png">
Karl Kraus 1933: Dritte Walpurgisnacht — edition view with expanded detail info
</Figure>

For the each of the annotations — listed in the stats table — at least one short info view and one detail info view is needed. In some cases an annotation contains multiple references.
During the first phase of the project a significant amount of time was invested into defining and refining the rule set for rendering prosographic and bibliographic data, as well as proofreading the final outcome.

### Test method

To ensure consistency throughout the migration process text comparison tests were setup.
Considering the number of annotations in the edition the most efficient way is to run the same test repeatedly with different params.
Its up to the developer or test manager which structure makes the most sense for a project.
In the presented project the test specifications are organized in directories per view - for the mentioned examples detail view and short info view -  and as specification file per entity type.

<Figure src="images/test_directory_structure.png">
Directory Structure for the test specifications
</Figure>

The pre-migration version of the application generates most of the data necessary for the mentioned views during its building process. This means that the data needed for the tests neither has to be gathered by repeatedly requesting the running application (pre-migration app) during testing nor by scraping single views or APIs.
The data already existed in JSON format and has been structurally adapted to a more test friendly schema.
To maintain a better overview this test data (being the test params) is provided in smaller junks: one file per edition page per view per entity type.

<Figure src="images/test_data.png">
grouping of the test data (test params) - samples
</Figure>

Each file contains an array of objects where each object represents a set of test params.

For each page the test script:
  1. Iterates through the array. 
  2. Sets the parameters.
  3. Executes the test.


A single test checking the correctness of a short info:
  1. Navigates to a page of the edition.
  2. Looks up the html element that contains the info.
  3. Compares its content to the expected text.

  <Figure src="images/short_info_test_person.png">
  Test steps for short info on person entity[^1]
  </Figure>

---
[^1]: the difference between 914 run tests and 899 annotations arises because a annotated string can refer to more than one entity.

 





